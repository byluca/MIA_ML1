{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbc34d4",
   "metadata": {},
   "source": [
    "# Creating Model Wrappers for Integration with *Scikit-Learn*\n",
    "\n",
    "In this notebook, we will demonstrate how to create a wrapper for a `Flux` model in Julia and use it in a `Pipeline` along with `GridSearchCV` to find the best hyperparameters. This procedure is identical for integrating any type of model. There are just two considerations to keep in mind.\n",
    "\n",
    "The first is that the model must implement the *ScikitLearnBase* interface, or at least, as we will see later, the necessary functions to be executable.\n",
    "\n",
    "The second consideration is that, since in Julia, Scikit-Learn is itself a wrapper for the Python package, once that package is called, you cannot return to Julia, and all models must return the results to Julia before proceeding. To give a clear example, if we tried to use GridSearchCV from Python, the model in Flux could not be integrated because the intermediate results are not accessible from Python to the Julia matrix. However, if we use the GridSearchCV package implemented in Julia, as we will see later, it does obtain the results from the Python packages, and therefore, it will work correctly.\n",
    "\n",
    "## 1. Installing Packages\n",
    "\n",
    "The first step is to ensure that the `Flux` library, which is a machine learning library for Julia, is installed.\n",
    "\n",
    "``` julia\n",
    "using Pkg\n",
    "Pkg.add([\"Flux\", \"ScikitLearn\", \"ScikitLearnBase\", \"Statistics\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ScikitLearnBase: BaseClassifier, fit!, predict, score, @declare_hyperparameters, is_classifier # Se explica más adelante\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9f7d4",
   "metadata": {},
   "source": [
    "## 2. Creating a Container Structure\n",
    "\n",
    "To create the *wrapper*, the first step in Python would be to create a class that inherits or adheres to a certain *interface*, meaning a set of method signatures that allow us to make calls uniformly and recognizable to the Scikit-Learn library.\n",
    "\n",
    "In Julia, unlike Python or other Object-Oriented languages, there are no classes as such. However, this is not a problem since it can be simulated with a mutable structure. These structures allow changing values and storing calls to different methods, similar to a class, and will also serve as a type for method overloading.\n",
    "\n",
    "The first step is importing the interface, in this case, `ScikitLearnBase`, which will provide all the method signatures needed. Below is an example with classification networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct ClassANN <: BaseClassifier\n",
    "    # Hyperparameters of the model (not learnable from data)\n",
    "    topology::AbstractVector{Int}\n",
    "    transferFunctions::AbstractVector{Function}\n",
    "    maxEpochs::Int\n",
    "    minLoss::Real\n",
    "    learningRate::Real\n",
    "\n",
    "    # Learnable Parameters (model in Flux and optimizer)\n",
    "    model::Chain\n",
    "    opt::ADAM\n",
    "\n",
    "    # Constructor which takes the hyperparameters as arguments\n",
    "    ClassANN(; topology=[1], transferFunctions=fill(σ, 1), maxEpochs=1000, minLoss=0.0, learningRate=0.01) =\n",
    "        new(topology, transferFunctions, maxEpochs, minLoss, learningRate, Chain(), ADAM(learningRate))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfeefc",
   "metadata": {},
   "source": [
    "In this code, you can see an initial section of parameters that will be required for the constructor, and it will be the user's responsibility to define them. Next, there are utility parameters that will be extracted from the data but will not be exposed externally, and finally, the definition of the constructor for this \"pseudo-class.\" The latter includes the default values for the set.\n",
    "\n",
    "To complete the definition, two additional things need to be defined. The first is a method that declares the type of problem the model can solve, in this case, classification. For this, the following call is necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point at ClassANN as a classifier\n",
    "is_classifier(::ClassANN) = true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3759b87",
   "metadata": {},
   "source": [
    "The second necessary element is to declare the parameters that the user needs to provide to construct an instance of this type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c83000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@declare_hyperparameters(ClassANN, [:topology, :transferFunctions, :maxEpochs, :minLoss, :learningRate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c70a5",
   "metadata": {},
   "source": [
    "# 3. Auxiliary Functions\n",
    "Next, we would define all the auxiliary functions necessary for the model's execution. This can be done within the calls to `fit!`, `predict`, and `score` themselves. Another option is to use functions that have been implemented in previous instances, such as those from the Machine Learning Fundamentals course. For this, you can use the following call:\n",
    "\n",
    "```julia\n",
    "include(\"MypackageFunctions.jl\")\n",
    "```\n",
    "\n",
    "However, for the sake of self-containment, below is an example code of the necessary functions for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83052b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform encoding, which receives the feature vector (one per pattern) and the classes\n",
    "function oneHotEncoding(feature::AbstractArray{<:Any,1}, classes::AbstractArray{<:Any,1})\n",
    "    # First, ensure that all elements in the feature vector are present in the classes vector\n",
    "    @assert(all([in(value, classes) for value in feature]))\n",
    "    numClasses = length(classes)\n",
    "    @assert(numClasses > 1)\n",
    "    if (numClasses == 2)\n",
    "        # If there are only two classes, return a matrix with one column\n",
    "        oneHot = reshape(feature .== classes[1], :, 1)\n",
    "    else\n",
    "        # If there are more than two classes, return a matrix with one column per class\n",
    "        # Either of these two types (Array{Bool,2} or BitArray{2}) works perfectly\n",
    "        # oneHot = Array{Bool,2}(undef, length(targets), numClasses)\n",
    "        # oneHot = BitArray{2}(undef, length(feature), numClasses)\n",
    "        oneHot = classes' .== feature\n",
    "    end\n",
    "    return oneHot\n",
    "end\n",
    "\n",
    "# This function is similar to the previous one, but if classes are not specified, they are taken from the feature variable itself\n",
    "oneHotEncoding(feature::AbstractArray{<:Any,1}) = oneHotEncoding(feature, unique(feature))\n",
    "\n",
    "# We overload the oneHotEncoding function in case a vector of boolean values is passed\n",
    "# In this case, the vector is already encoded, so we simply convert it into a column matrix\n",
    "oneHotEncoding(feature::AbstractArray{Bool,1}) = reshape(feature, :, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4317e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Constructing the Artificial Neural Network\n",
    "function buildClassANN(numInputs::Int, topology::AbstractVector{Int}, numOutputs::Int;\n",
    "            transferFunctions::AbstractVector{Function}=fill(σ, length(topology)))\n",
    "    layers = []\n",
    "    numInputsLayer = numInputs\n",
    "\n",
    "    for (i, numNeurons) in enumerate(topology)\n",
    "        push!(layers, Dense(numInputsLayer, numNeurons, transferFunctions[i]))\n",
    "        numInputsLayer = numNeurons\n",
    "    end\n",
    "\n",
    "    if numOutputs == 1\n",
    "        push!(layers, Dense(numInputsLayer, 1, σ))\n",
    "    else\n",
    "        push!(layers, Dense(numInputsLayer, numOutputs, identity))\n",
    "        push!(layers, softmax)\n",
    "    end\n",
    "\n",
    "    return Chain(layers...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908dec5",
   "metadata": {},
   "source": [
    "We might also need the `trainANNClass` function, but in this case, for purely educational purposes, a simple implementation will be provided directly within the function to serve as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70524561",
   "metadata": {},
   "source": [
    "## 4. Implementing the *Scikit-Learn* Functions\n",
    "\n",
    "As previously mentioned, it is necessary to implement the methods required by ScikitLearnBase. For classification, these include `fit!`, `predict`, and `score`.\n",
    "\n",
    "First, the `fit!` function is used for fitting the model using Flux for training. If it is separated into a function, it can be utilized accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e3951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the model fitting (fit!)\n",
    "function fit!(model::ClassANN, X, y)\n",
    "    numInputs = size(X, 2)\n",
    "    numOutputs = length(unique(y))\n",
    "\n",
    "    # Build the model using Flux\n",
    "    model.model = buildClassANN(numInputs, model.topology, numOutputs, transferFunctions=model.transferFunctions)\n",
    "    model.opt = ADAM(model.learningRate)\n",
    "\n",
    "    # Convert the labels using one-hot encoding if the classification is  multiclass\n",
    "    if numOutputs > 1\n",
    "        y = oneHotEncoding(y, unique(y))\n",
    "    end\n",
    "\n",
    "    # Define the loss function\n",
    "    loss(x, y) = Flux.crossentropy(model.model(x), y)\n",
    "\n",
    "    # Train the model\n",
    "    # Alternatively\n",
    "    #(model, results) = trainANNClass(model.topology, (X', y')], model.maxEpoachs, model.minLoss, model.learningRate)\n",
    "    for epoch in 1:model.maxEpochs\n",
    "        Flux.train!(loss, Flux.params(model.model), [(X', y')], model.opt)\n",
    "        current_loss = loss(X', y')\n",
    "        println(\"Epoch: $epoch, Loss: $current_loss\")\n",
    "        if current_loss <= model.minLoss\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73690f",
   "metadata": {},
   "source": [
    "It is also required the implementation of `predict` for the use in crossvalidation and pipeline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the models prediction (predict)\n",
    "function predict(model::ClassANN, X)\n",
    "    if size(model.model(X'), 1) > 1\n",
    "        return Flux.onecold(model.model(X'), 1:size(model.model(X'), 1))\n",
    "    else\n",
    "        return round.(model.model(X'))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee0674",
   "metadata": {},
   "source": [
    "It will also be required the `score`function, becasuse we are going to use this *wrapper* with `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función adicional para calcular el puntaje (score) del modelo\n",
    "function score(model::ClassANN, X, y)\n",
    "    predictions = predict(model, X)\n",
    "    return mean(predictions .== y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430ff14",
   "metadata": {},
   "source": [
    "### 4.1 Transformadores y regresión\n",
    "\n",
    "En caso de emplearse un wrapper de otro método como puede ser un PCA o alguna técnica de aprendizaje no supervisado. Sería necesrio implementar las funciones `fit!`, `transform` y `fit_transform`.\n",
    "POr su parte en el caso de implementar un modelo de regresión, sería preciso implementar `fit!`, `predict` `score`y en la mayoría de los casos `predict_proba`. Consulte el API en [Scikit Learn Model API](https://scikitlearnjl.readthedocs.io/en/latest/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee524dcc",
   "metadata": {},
   "source": [
    "# 5. Interaction with the Library\n",
    "\n",
    "The previous code can be saved in a file and loaded using an `include` statement or through a module that can be used later. In that case, remember to export the necessary functions from the module.\n",
    "\n",
    "In any case, the following code also demonstrates how to call these functions and how to integrate them within a `Pipeline`. You can modify it to search for the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa37b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.jl\n",
    "\n",
    "using ScikitLearn\n",
    "using ScikitLearn.Pipelines: Pipeline, named_steps\n",
    "using ScikitLearn.GridSearch: GridSearchCV         #IMPORTANT use the Julia Implemetation not the Python one\n",
    "#@sk_import model_selection: GridSearchCV          # This implementation is the Python one and would give an error becase it can not found the Wrapper\n",
    "\n",
    "@sk_import decomposition: PCA\n",
    "@sk_import datasets: load_iris\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# Define the Grid (hiperparameters to test)\n",
    "param_grid = Dict(\n",
    "    \"ann__maxEpochs\" => [500, 1000], \n",
    "    \"ann__learningRate\" => [0.01, 0.1]\n",
    ")\n",
    "\n",
    "# Define the base parámeters\n",
    "topology = [3, 4]\n",
    "functions = [σ, σ]\n",
    "maxEpochs = 1000\n",
    "minLoss = 0.0\n",
    "learningRate = 0.01\n",
    "\n",
    "# create a pipeline with a model included in the wrapper\n",
    "ann = ClassANN(topology=topology, transferFunctions=functions, maxEpochs=maxEpochs, minLoss=minLoss, learningRate=learningRate)\n",
    "\n",
    "estimators = [(\"pca\",PCA()),(\"ann\",ann)]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Setup the GridSearchCV\n",
    "grid_search = GridSearchCV(pipe, param_grid)\n",
    "\n",
    "# Train the model using GridSearchCV\n",
    "fit!(grid_search, X, y)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "println(\"Best model: \", grid_search.best_estimator_)\n",
    "println(\"Best hiperparameters: \", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
