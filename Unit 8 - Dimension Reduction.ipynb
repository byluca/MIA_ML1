{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a35ab0",
   "metadata": {},
   "source": [
    "## The curse of dimensionality\n",
    "\n",
    "One of the main problems in machine learning is what is known as the curse of dimensionality or the curse of dimensions. This problem arises from the moment in which one tries to improve an approximation by simply using more variables. Maybe you could, but most probably you can't. This is where the curse of dimensionality comes in. It is here that the curse is incurred since, **as the number of characteristics or dimensions increases, the amount of data needed to obtain an accurate generalisation increases exponentially. ** However, before you can tackle the problem, you need to know where it comes from.\n",
    "\n",
    "![Performance behaviour when the number of variables is increased](./img/unit8/Perfomance_Dimension_plot.png)\n",
    "\n",
    "\n",
    "### The dimensionality of problems\n",
    "\n",
    "Rarely do the impact that a given variable will have on an optimisation process come to mind. Take for example a case where there are 5 observations for a given variable **X**, and these observations are uniformly distributed in space. Thus each of the observations will try to represent $\\frac{1}{5}$ of the said space.\n",
    "\n",
    "![](./img/unit8/Sampling_Examples.png)\n",
    "\n",
    "hen adding another **Y** variable (moving to a two-dimensional space) in order to maintain the same distance between them, i.e. the same rate of representativeness of the space, the number of samples should be increased to 25. With a third one, 125 samples would be needed to explore the space under the same conditions, and so on. \n",
    "\n",
    "Therefore, the present problem increases exponentially the more dimensions we have.\n",
    "\n",
    "\n",
    "### The curse\n",
    "\n",
    "What happens in a real problem? Well, it is usually not possible to increase the number of samples to maintain the representativeness of the sample points and the equidistance. That is why, if we add a new feature but do not provide enough points, the result would be a more complex model but with an impoverished performance.\n",
    "\n",
    "The reason for this statement can be clearly seen in the following example in which we try to classify between images of cats and dogs. If only one dimension is taken into account, the examples are uniformly distributed. \n",
    "\n",
    "![](./img/unit8/Doom_1.png)\n",
    "\n",
    "In this case, there are 10 samples covering the whole space. But, if you increase one dimension, that distribution becomes something like the following figure.\n",
    "\n",
    "![](./img/unit8/Doom_2.png)\n",
    "\n",
    "This situation might lead one to think that a new dimension would make it even easier to divide the space. For example, adding a third dimension to the previous problem would give something like:\n",
    "\n",
    "![](./img/unit8/Doom_3.png)\n",
    "\n",
    "Where the result is linearly separable as can be seen in the following image.\n",
    "\n",
    "![](./img/unit8/Doom_4.png)\n",
    "\n",
    "The erroneous conclusion that can be drawn is that the higher the dimensionality is increased, the easier the separation on the basis of features will be. Note how the distribution of the data has changed: while in one dimension there are 2 samples per five-sample interval mentioned above, in three-dimensional space there are barely 0.08 samples per interval (10/125). Therefore, it is more complicated to find counter examples on the same side of the classifier. The problem arises when we project this data to a lower dimensional space as happens when any artificial neural network is applied when creating a classifier. In that situation the result would be similar to the following figure:\n",
    "\n",
    "![](./img/unit8/Doom_5.png)\n",
    "\n",
    "As can be seen in the following figure, the classifier has been overtrained and therefore the result is not as good as it could be for new instances. For example, see the following figure where a simple linear classifier has been applied on fewer dimensions\n",
    "\n",
    "![](./img/unit8/Doom_6.png)\n",
    "\n",
    "### How to avoid the curse?\n",
    "\n",
    "There is no single rule defining how many features should be used in a regression/classification problem. The number will depend on the amount of training data available, the complexity of the decision boundaries and the type of classifier being used. \n",
    "\n",
    "There are mainly two types of approaches in order to reduce dimensionality. These two types are:\n",
    "* projections \n",
    "* transformations\n",
    "\n",
    "The difference between one and the other is that, while projections operate on the space defined by the input samples itself, transformations try to modify this space in order to find a transition function that allows a suitable and separable representation of the data.\n",
    "Some of the most common techniques are:\n",
    "\n",
    "* Principal Component Analysis (PCA) \n",
    "* Linear Discriminant Analysis (LDA) \n",
    "* Independent Components Analysis (ICA) \n",
    "* Locally linear embedding (LLE) \n",
    "* T-distributed Stochastic Stochastic Neighbor Embedding (t-SNE) \n",
    "* IsoMaps\n",
    "* Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f062de",
   "metadata": {},
   "source": [
    "## Loading the data for the examples\n",
    "In this unit we are going to load, again, the machine learning problem known as **rock or mine?**. As it was said in the previous unit, this small database consists of 111 patterns corresponding to rocks and 97 patterns corresponding to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements between 0.0. and 1.0. These measurements correspond to the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "The first step will be to download the data set if it is not already available. For this purpose, the following code will be used with the utility function shown below. You should already have it downloaded, but as an alternative to the previous case where we use the DataFrames to prepare the data in this case we are going to use pure default libraries and Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Downloads;\n",
    "using DelimitedFiles;\n",
    "\n",
    "function load_data(filename::String, url::String)\n",
    "    if !isfile(filename)\n",
    "        Downloads.download(url, filename)\n",
    "    end\n",
    "    data = readdlm(filename,',');\n",
    "end\n",
    "\n",
    "file_name = \"sonar.all_data\"\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = load_data(file_name, url)\n",
    "size(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4ede5",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Does the fact that the values of the variables are between 0 and 1 imply that they are standardised?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5eba90",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863719d7",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "Once the data have been downloaded, although the measurements are already normalised, although they are not standardised, it is necessary to make a small adaptation to the data. Specifically, it is necessary to divide the set into inputs and outputs, as well as to change the interpretation of the last column, which contains the problem label, to an integer. In addition, the data set will also be divided into training and test. This division will maintain a percentage of 10% of patterns for the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58342a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load your library of functions\n",
    "include(\"Your library goes in here.jl\")\n",
    "\n",
    "\n",
    "# Prepare input data\n",
    "input_data = normalizeMinMax!(convert(Array{Float32,2},data[:,1:end-1]))\n",
    "@assert isa(input_data, Array{<:Float32,2})\n",
    "\n",
    "# Prepare output data\n",
    "output_data = oneHotEncoding!(data[:,end])\n",
    "@assert isa(output_data, AbstractArray{Bool,2})\n",
    "\n",
    "# Split the data in train and test dataset\n",
    "# TODO\n",
    "train_input, train_output = ...\n",
    "test_input, test_output = ...\n",
    "\n",
    "@assert size(train_input) ≈ [floor(Int, 0.9*size(input_data, 1)), size(input_data,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54eded0",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Probably the most widely used dimensionality reduction technique. It can be used both individually and in combination with other techniques. It is a method that transforms data by means of a projection onto a set of orthogonal axes. To do this, the method searches for the best linear combinations of the original variables, trying to maximise the variance along the new variable. For example, the image on the right shows a set of points in three dimensions. These three dimensions, when projected, give the images on the right that allow us to see the variability of the data for each of the axes. The continuous line is the one with the highest variability and, therefore, the one that will be taken as the base or first component. For the second component, among the remaining possibilities, the one that maximises and remains perpendicular (orthogonal) to the first dimension selected will be chosen.\n",
    "\n",
    "![](./img/unit8/PCA.png)\n",
    "\n",
    "If a third dimension is needed, PCA would have to search for one that is perpendicular to these. This process is based on the so-called *Single Value Decomposition (SVD)* matrix which extracts the eigenvectors from the sample space. These are sorted in decreasing order and those that best represent the corresponding space are selected.\n",
    "\n",
    "For those who wish to understand in detail how it works, a step-by-step description of how to implement PCA can be found in the following [link](https://sebastianraschka.com/Articles/2014_pca_step_by_step.html).\n",
    "\n",
    "Generally speaking, if you want to make use of PCA, a good alternative is to use the implementation in the package `MultivariateStats` library. This library has the `PCA` function that allows us to execute this reduction technique. It can be use as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76adfd0-4a7a-4046-a891-666bf7474366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To execurte a single time\n",
    "Using Pkg;\n",
    "Pkg.add('MultivariateStats.jl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f681b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "\n",
    "MLJ.@load PCA pkg=MultivariateStats\n",
    "\n",
    "#Define the PCA object and the number of components that are desired\n",
    "pca_model = PCA(maxoutdim=2)\n",
    "\n",
    "# Create the machine and fit it on the training data\n",
    "pca_mach = machine(pca_model, MLJ.table(train_input))\n",
    "fit!(pca_mach)\n",
    "\n",
    "#Once it is adjusted it can be used to transform the data\n",
    "pca_train = transform(pca_mach, MLJ.table(train_input))\n",
    "pca_test = transform(pca_mach, MLJ.table(test_input))\n",
    "\n",
    "#Check the shapes\n",
    "@assert (size(train_input)[1],2) == size(pca_train)\n",
    "@assert (size(test_input)[1],2) == size(pca_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a26dd8",
   "metadata": {},
   "source": [
    "It should be highlighted the importance of fitting the transformation on the training data only. In case of doing it on the total data, the transformation would be contaminating with the transformation the possible training of classification or regression techniques that could be applied later.\n",
    "\n",
    "One of the main advantages of applying dimensionality reduction is that it allows a first visual study to be carried out by transforming a multidimensional space into a 2 or 3 dimensional space that can be represented graphically. In the example, the PCA-transformed data will be used for the representation. First, a function is defined to facilitate the presentation of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots;\n",
    "\n",
    "function draw_results(x, y; colors, target_names=nothing)\n",
    "    num_classes = length(unique(colors))\n",
    "\n",
    "    if !isnothing(target_names)\n",
    "        @assert num_classes == length(target_names)\n",
    "        label = target_names\n",
    "    else\n",
    "        label = [string(i) for i in 1:num_classes]\n",
    "    end\n",
    "\n",
    "    fig = plot()\n",
    "    if (num_classes == 2)\n",
    "        possitive_class = y[:,1].==1\n",
    "        scatter!(fig, x[possitive_class,1], x[possitive_class,2], markercolor=colors[1], label=label[1])\n",
    "        scatter!(fig, x[.!possitive_class,1], x[.!possitive_class,2], markercolor=colors[2], label=label[2])\n",
    "    else\n",
    "        for i in 1:num_classes\n",
    "            index_class = y[:,i].==1\n",
    "            scatter!(fig, x[index_class, 1], x[index_class, 2], markercolor=colors[i], label=label[i])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780e795e",
   "metadata": {},
   "source": [
    "Without dimensionality reduction, the expert would be responsible for choosing the two variables to represent. Doing so would run the risk of not representing the distribution correctly. Then, print two of the dimensions of your choice and compare the results with the one obtained by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f554ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_results(pca_train, train_output, colors=[:green,:red], target_names=[\"Rock\", \"Mine\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c8fbf",
   "metadata": {},
   "source": [
    "A reduction to 2 or 3 dimensions can be helpful when trying to make a first analysis to, for example, determine whether a linear classifier can give good results or whether there is a pattern in the distribution of the data. However, it is more usual to try to reduce the dimensionality while keeping as much variability as possible. To do this, the scikit-learn function allows you to pass a value between 0 and 1, which determines the percentage of variability that should be maintained. A typical value is 0.95 as it maintains almost all the relevant information and eliminates most of the noise. For example the same examble as before but making a reduction of 95% should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA keeping 95% of the total variance\n",
    "pca_model = PCA(pratio=0.95)\n",
    "\n",
    "# Train the PCA model\n",
    "pca_mach = machine(pca_model, MLJ.table(train_input))\n",
    "fit!(pca_mach)\n",
    "\n",
    "# Transform the data\n",
    "pca_train = transform(pca_mach, MLJ.table(train_input))\n",
    "pca_test = transform(pca_mach, MLJ.table(test_input))\n",
    "\n",
    "print(\"Train Patterns \", size(train_input), \" -> \", size(pca_train))\n",
    "print(\"Train Patterns \", size(test_input), \" -> \", size(pca_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9945296",
   "metadata": {},
   "source": [
    "Apart from being able to represent the information, reducing the dimensionality is often associated with an acceleration of training. This is because the computational complexity and computational effort of most learning algorithms is conditioned by the number of variables. In addition, there is also often an improvement in the models when some of the noise is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJBase: accuracy\n",
    "\n",
    "MLJ.@load SVMClassifier pkg=LIBSVM\n",
    "MLJ.@load LogisticClassifier pkg=MLJLinearModels\n",
    "MLJ.@load DecisionTreeClassifier pkg=DecisionTree\n",
    "MLJ.@load GaussianNB pkg=NaiveBayes\n",
    "\n",
    "models = Dict(\n",
    "    \"SVM\" => SVMClassifier(),\n",
    "    \"LR\"  => LogisticClassifier(),\n",
    "    \"DT\"  => DecisionTreeClassifier(max_depth=4),\n",
    "    \"NB\"  => GaussianNB()\n",
    ")\n",
    "\n",
    "train_y = categorical(train_output)\n",
    "test_y = categorical(test_output)\n",
    "\n",
    "@time begin\n",
    "    for (name, model) in models\n",
    "        mach = machine(model, train_input, train_y)\n",
    "        fit!(mach)\n",
    "        ŷ = predict(mach, test_input)\n",
    "\n",
    "        # Convert probabilistic outputs if needed \n",
    "        #SVM and GaussianNB return probabilities, so it needs the \"mode.\"\n",
    "        if ŷ isa Vector{UnivariateFinite}\n",
    "            ŷ = mode.(ŷ)\n",
    "        end\n",
    "\n",
    "        acc = accuracy(ŷ, test_y)\n",
    "        println(\"$name: $(acc*100) %\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5952e",
   "metadata": {},
   "source": [
    "### Question 8.1\n",
    "> ❓ Compare the results after applying PCA on the data. To do that, you have to implement the same train process as in the previous piece of code but this time using the PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "\n",
    "#TODO\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7319f",
   "metadata": {},
   "source": [
    "So, when using the `PCA` model from [`MultivariateStats.jl`](https://juliastats.org/MultivariateStats.jl/stable/) through MLJ, the following keyword arguments can be configured:\n",
    "\n",
    "- `maxoutdim::Int`:  \n",
    "  Specifies the maximum number of principal components to compute.  \n",
    "  Useful when you want a fixed number of reduced dimensions.\n",
    "\n",
    "- `pratio::Float64`:  \n",
    "  Proportion of total variance to preserve (between 0 and 1).  \n",
    "  The model will automatically select the number of components required to reach this threshold.  \n",
    "  **Use either `maxoutdim` or `pratio`, but not both.**\n",
    "\n",
    "- `method::Symbol`:  \n",
    "  SVD algorithm used for computation. Options are:\n",
    "  - `:auto` (default): chooses based on matrix size.\n",
    "  - `:svd`: full singular value decomposition.\n",
    "  - `:eigen`: also known as truncated is based on eigenvalue decomposition of the covariance matrix.\n",
    "\n",
    "- `mean::Bool`:  \n",
    "  Whether to center the data before computing PCA (default: `true`).  \n",
    "  Set to `false` if data has already been mean-centered.\n",
    "\n",
    "- `whiten::Bool`:  \n",
    "  Whether to whiten the components (i.e., scale them to unit variance) after projection. Default is `false`.\n",
    "\n",
    "- `tol::Float64`:  \n",
    "  Tolerance threshold for discarding small singular values (default: machine epsilon).\n",
    "\n",
    "\n",
    "From those parameters, something important to highlight is the method used to compute the singular value decomposition (SVD) of the data matrix.Depending on if it is a full SVD or a Truncated SVD the following condition is applied:\n",
    "```julia\n",
    "    if n_samples ≥ n_features\n",
    "        method = :eigen\n",
    "    else\n",
    "        method = :svd\n",
    "    end\n",
    "```\n",
    "\n",
    "In Julia, other algorithms are available depending on the library used. By default, `MultivariateStats.jl` — the backend used by MLJ’s PCA model — relies on LAPACK for computing the full or truncated SVD. This method is efficient for small to medium-sized matrices and provides numerically stable results. For larger datasets or when incremental learning is required, other Julia packages offer alternatives:\n",
    "\n",
    "- **Randomized SVD**: The `RandomizedSVD.jl` package (not currently integrated in MLJ) provides efficient approximations of truncated SVD using randomized methods, suitable for large or sparse datasets. While this is not yet wrapped in MLJ, it can be used separately or integrated into pipelines manually.\n",
    "\n",
    "- **Kernel PCA**: The nonlinear extension of PCA can be implemented using the kernel methods available in Julia, such as those provided by `KernelFunctions.jl` or `MLKernels.jl`. Although not available in a drop-in PCA form, a kernel transformation followed by linear PCA can be composed manually. This is currently available in other ML packages in other languages such as Python in `scikit-learn`.\n",
    "\n",
    "- **Incremental PCA**: Julia supports online learning via packages like `OnlineStats.jl`, which provide incremental versions of PCA and related models. These are suitable for streaming or memory-constrained settings.\n",
    "\n",
    "These alternatives allow users to choose the most appropriate PCA variant depending on dataset size, memory constraints, and desired properties of the resulting components.\n",
    "\n",
    "[1] Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). \"Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions\". SIAM review, 53(2), 217-288](https://doi.org/10.1137/090771806)\n",
    "\n",
    "[2] Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). \"A randomized algorithm for the decomposition of matrices\". Applied and Computational Harmonic Analysis, 30(1), 47-68](https://doi.org/10.1016/j.acha.2010.02.003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745d85e1",
   "metadata": {},
   "source": [
    "## Independent Component Analysis (ICA)\n",
    "\n",
    "Although it is often not referred to as a dimensionality reduction technique, but rather as a principal component extraction technique, it is probably the second most popular technique applied in this sense. ICA is also a linear dimension reduction method, which transforms the dataset into columns of independent components. Blind source separation and the \"cocktail party problem\" are other names for it. ICA is an important tool in neuroimaging, fMRI and EEG analysis that helps to separate normal signals from abnormal ones.\n",
    "\n",
    "In the case of this algorithm, it assumes that the data presented is the result of a linear combination of two inputs and that neither of them has a Gaussian distribution. If this condition is not met, the results will not be good or will be inconsistent.\n",
    "\n",
    "Without going too much into the mathematical underpinnings, it should be noted that problems arising from non-linear dependencies or having a Gaussian distribution can be minimised by the use of entropy in the formulation of the algorithm.\n",
    "In general terms, the ICA pseudocode can be summarised as follows:\n",
    "```\n",
    "Initialise W\n",
    "X = PCA(X)\n",
    "While W changes:\n",
    "      W = average(X*G(WX)) - average(g(WTX))W\n",
    "      W = orthogonalize(W)\n",
    "return S = WX\n",
    "```\n",
    "\n",
    "Where $W$ is the weight matrix used to project the original data, $G$ is the generalised entropy function (measuring the entropy difference between components), and $g$ is its derivative. The `orthogonalize` step refers to the process of making the matrix columns orthogonal during the unmixing stage. In Julia, this process can be efficiently implemented using the `FastICA` model provided by [`MultivariateStats.jl`](https://juliastats.org/MultivariateStats.jl/stable/), which is also available through the MLJ framework. This implementation allows for dimensionality reduction by maximising statistical independence between components.\n",
    "\n",
    "We now demonstrate how to apply `FastICA` to our dataset using MLJ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7be4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.@load FastICA pkg=MultivariateStats\n",
    "\n",
    "ica_model = FastICA(maxoutdim=2)\n",
    "ica_mach = machine(ica_model, MLJ.table(train_input))\n",
    "fit!(ica_mach)\n",
    "\n",
    "ica_train = transform(ica_mach, MLJ.table(train_input))\n",
    "ica_test = transform(ica_mach, MLJ.table(test_input))\n",
    "\n",
    "println(\"Train Patterns \", size(train_input), \" -> \", size(ica_train))\n",
    "println(\"Test Patterns \", size(test_input), \" -> \", size(ica_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cc810e-cfc9-42e2-9092-f7be8b72f3de",
   "metadata": {},
   "source": [
    "Additionaly, using `FastICA` through MLJ, the following keyword arguments can be specified during model construction:\n",
    "\n",
    "- `maxoutdim::Int`:  \n",
    "  The number of independent components to extract (i.e., the reduced dimensionality).  \n",
    "  **This is the most common parameter to adjust.**\n",
    "\n",
    "- `algorithm::Symbol`:  \n",
    "  The algorithm used for contrast function optimization. Options are `:parallel` (default) or `:deflation`.\n",
    "\n",
    "- `contrast::Function`:  \n",
    "  The contrast (non-linearity) function used to approximate neg-entropy. Common choices include:\n",
    "  - `gpow3`: (default) $g(u) = u^3$\n",
    "  - `gtanh`: $g(u) = \\tanh(u)$\n",
    "  - `ggaus`: $g(u) = u \\cdot \\exp(-u^2 / 2)$\n",
    "\n",
    "- `maxiter::Int`:  \n",
    "  Maximum number of iterations for convergence (default: 1000).\n",
    "\n",
    "- `tol::Float64`:  \n",
    "  Convergence tolerance (default: `1e-8`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad99c3a",
   "metadata": {},
   "source": [
    "### Question 8.2\n",
    "> ❓ Compare the results of both the ICA and PCA approaches. Identify which one is easier to classify. In the following section, plot the output of the ICA approach and compare it with the PCA output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458315dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output of the ICA transformation\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e2f1b",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ddf5c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Repeat the train of the classifiers and compare the results with the two previous approaches. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the training of previous approaches\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4374845",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe880898",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "This linear machine learning algorithm is used for multi-class classification, although it is sometimes used as a dimensionality reduction algorithm. Beware of confusing it with Latent Dirichlet Allocation (LDA), which is also a dimensionality reduction technique but only applicable to text documents.\n",
    "\n",
    "LDA tries to separate (or discriminate) as best as possible the samples of the training dataset by their class value. Specifically, the model tries to find a linear combination of input variables that achieves the maximum separation of samples between classes (centroids or class means) and the minimum separation of samples within each class. Therefore, its major difference with PCA is that LDA takes the output class into account, while PCA is completely agnostic to this fact.\n",
    "\n",
    "The implementation of Linear Discriminant Analysis (LDA) in Julia, available through the MLJ interface to `MultivariateStats.jl`, allows both **classification** and **dimensionality reduction**.\n",
    "\n",
    "In this notebook, we will focus on using LDA as a **feature extraction technique**, projecting the data onto a lower-dimensional subspace that maximizes class separability.\n",
    "\n",
    "Below is an example showing how to apply LDA to transform a dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLJ.@load LDA pkg=MultivariateStats\n",
    "\n",
    "# Instantiate the LDA model\n",
    "lda_model = LDA()\n",
    "\n",
    "# Train the model on input features and categorical targets\n",
    "# Beaware then the output has to be converted in categorical\n",
    "# also the reduction would be C-1 where C is the number of classes\n",
    "# So it is not suitable for binary classification problems\n",
    "lda_mach = machine(lda_model, MLJ.table(train_input), categorical(train_output))\n",
    "fit!(lda_mach)\n",
    "\n",
    "# Transform the data\n",
    "lda_train = transform(lda_mach, MLJ.table(train_input))\n",
    "lda_test = transform(lda_mach, MLJ.table(test_input))\n",
    "\n",
    "println(\"Train Patterns \", size(train_input), \" -> \", size(lda_train))\n",
    "println(\"Test Patterns \", size(test_input), \" -> \", size(lda_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4eee5e",
   "metadata": {},
   "source": [
    "The `LDA` model provided by [`MultivariateStats.jl`](https://juliastats.org/MultivariateStats.jl/stable/) and available through MLJ supports the following keyword arguments:\n",
    "\n",
    "- `regparam::Float64`  \n",
    "  Regularization parameter used to stabilize the within-class covariance matrix.  \n",
    "  A small positive value (e.g. `0.01`) can help in cases where classes are highly imbalanced or nearly collinear.\n",
    "\n",
    "- `pratio::Float64`  \n",
    "  Proportion of total discriminant directions to retain (between 0 and 1).  \n",
    "  By default, all possible directions (`min(#features, #classes - 1)`) are retained.  \n",
    "  Use this to reduce dimensionality while preserving most of the class-separating variance.\n",
    "\n",
    "- `maxoutdim::Int`  \n",
    "  Alternatively, you can explicitly specify the number of output dimensions.  \n",
    "  This must be less than or equal to `min(#features, #classes - 1)`.\n",
    "\n",
    "- `tol::Float64`  \n",
    "  Numerical tolerance used in internal matrix inversions. Usually not modified.\n",
    "\n",
    "\n",
    "> **Note:**  \n",
    "> LDA can be used in two different ways:\n",
    ">\n",
    "> - As a **transformer**: It projects the input data onto a lower-dimensional space that best separates the classes. This is useful as a preprocessing step for other models.\n",
    ">\n",
    "> - As a **classifier**: LDA can also directly assign class labels to new observations based on the model's probabilistic assumptions.\n",
    ">\n",
    "> In the context of `MultivariateStats.jl`, LDA is primarily used for **dimensionality reduction** via the `transform` function. For classification purposes, other models in MLJ (such as `LinearDiscriminantClassifier`) may be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3995a2",
   "metadata": {},
   "source": [
    "## t-Distributed Stochastic Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "It is a **nonlinear dimensionality reduction** technique, commonly used for the **visualization** of high-dimensional data. Unlike PCA, which is a linear method that preserves global structures by maximizing variance and separating distant points, t-SNE focuses on **preserving local structure** — that is, it keeps similar points close together in the projected space.\n",
    "\n",
    "![Comparison of distributions](./img/unit8/t-SNE.png) \n",
    "\n",
    "The algorithm operates by:\n",
    "\n",
    "1. **Computing pairwise similarities** in the high-dimensional space using **Gaussian distributions**.\n",
    "2. **Computing pairwise similarities** in the low-dimensional space using a **Student’s t-distribution** (a simplification of the Cauchy distribution).\n",
    "3. Minimizing the **Kullback-Leibler (KL) divergence** between these two distributions via **gradient descent**.\n",
    "\n",
    "The similarity of one point to another is defined as the conditional probability that a point \\( A \\) would pick point \\( B \\) as its neighbor, assuming neighbors are picked proportionally to a Gaussian density centered at \\( A \\).\n",
    "\n",
    "By minimizing the KL divergence between high- and low-dimensional similarities, t-SNE creates an embedding that preserves local neighborhoods.\n",
    "\n",
    "> 🔸 KL divergence is also used in other probabilistic models such as Variational Autoencoders (VAE) and GANs.\n",
    "\n",
    "\n",
    "#### Important considerations\n",
    "\n",
    "- The **axes in the t-SNE output have no meaning** — distances and directions are not interpretable beyond local neighborhood structure.\n",
    "- The **input features are not preserved** and cannot be inferred from the output.\n",
    "- **t-SNE is not a model**: it does not learn a reusable transformation like PCA or LDA.\n",
    "- **You must apply it independently** to each dataset (e.g., training and test sets), which may yield slightly different distributions.\n",
    "\n",
    "\n",
    "t-SNE is computationally expensive for very high-dimensional data (> 50 features).  \n",
    "It is common to **apply PCA first** to reduce dimensionality, and then run t-SNE on the reduced space to improve both **performance** and **stability**.\n",
    "\n",
    "Although t-SNE is not currently integrated in MLJ, it can be used in Julia via the [`TSne.jl`](https://github.com/lejon/TSne.jl) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "Pkg.add('TSne.jl');\n",
    "\n",
    "using TSne\n",
    "\n",
    "# Convert matrices to Float32 as required by TSne.jl\n",
    "train_input_f32 = convert(Matrix{Float32}, train_input)\n",
    "test_input_f32 = convert(Matrix{Float32}, test_input)\n",
    "\n",
    "# Apply t-SNE with 2 output dimensions\n",
    "tsne_train = tsne(train_input_f32, 2)\n",
    "tsne_test = tsne(test_input_f32, 2)\n",
    "\n",
    "println(\"Train Patterns \", size(train_input), \" -> \", size(tsne_train))\n",
    "println(\"Test Patterns \", size(test_input), \" -> \", size(tsne_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2898817",
   "metadata": {},
   "source": [
    "The function `tsne` accepts the following parameters:\n",
    "\n",
    " * `data::Matrix{Float32}`: input data\n",
    "\n",
    " * `no_dims::Int:` target dimension (usually 2 or 3)\n",
    "\n",
    " * `perplexity`: balance between local and global structure (default: 30)\n",
    "\n",
    " * `max_iter`: maximum number of iterations (default: 1000)\n",
    "\n",
    " * `learning_rate`: step size in gradient descent (default: 200)\n",
    "\n",
    "> Note:\n",
    "Since t-SNE is unsupervised and non-parametric, the `tsne(...)` function does not support `.transform` or `.fit!` calls, and the transformation cannot be reused on other datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f17a1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
